{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DLRiiUXHHZTT",
        "5aG8iUVmHZTW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sa1syo/NLTK/blob/main/IRNLP2019_Ex11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RcVoQseHZTN"
      },
      "source": [
        "# Exercise 11. Information Retrieval 1\n",
        "\n",
        "Exercise 11 and 12 are original contents of IR&NLP in Univ. of Aizu.\n",
        "Please read carefully in this page and try their source code.\n",
        "\n",
        "\n",
        "本演習では、検索エンジンを作るための簡単な例として、今まで勉強してきたNLTKの扱い方を駆使して、簡単な情報検索システムを作成しましょう。  \n",
        "In this exercise, we try to make a kind of document search engine utilize NLTK as to construct search engine.\n",
        "\n",
        "検索対象は世界的なベストセラーの聖書(Bible-KJV)の中に含まれる66の書物、計1183章、31173節あります。  \n",
        "Target document is Bible-KJV (it has 66 books, 1183 chapters and 31173 sections in total) which is a best-seller book in world wide.\n",
        "\n",
        "ここでは、書物毎, 行（凡そ節ごと、但し連接している場合は1つの行とみなす）毎のWord Count及びTF\\*IDFを学びます。  \n",
        "We study about how to index all the documents using word count and TF\\*IDF in each books and lines (or chapters).\n",
        "\n",
        "\n",
        "## 11.1. Preprocessing\n",
        "- 対象とするデータ: Gutenberg Corpusから, Bible-KJV  \n",
        " Target Data: Bible-KJV from Gutenberg Corpus. (Please remember how to load a book from Gutenberg Corpus in Ex.2)\n",
        "- UTF-8で記述されているRawデータから, Parsing, Stemmingを経て、Stemmingの掛かったTextオブジェクトへの変換を行う。  \n",
        " Convert the text object wihch processed parsing and lemmatizing from raw data which is UTF-8.\n",
        "  - http://www.gutenberg.org/cache/epub/10/pg10.txt\n",
        "\n",
        "In AY2020, urllib is not able to move enough, then we will use \"requests\" module.  \n",
        "If you need some information about urllib, please go to Chapter 3, to download from gutenberg.org"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg8xio0zHZTO"
      },
      "source": [
        "import requests\n",
        "r = requests.get('http://www.gutenberg.org/cache/epub/10/pg10.txt')\n",
        "raw = r.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-bC5R4tHZTP",
        "scrolled": true,
        "outputId": "8310e190-f27d-4110-bfc3-bb424cbc0cea"
      },
      "source": [
        "# 平文1000文字分を表示\n",
        "# Print 1000 characters\n",
        "print(raw[:1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of The King James Bible\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
            "of the Project Gutenberg License included with this eBook or online at\r\n",
            "www.gutenberg.org. If you are not located in the United States, you\r\n",
            "will have to check the laws of the country where you are located before\r\n",
            "using this eBook.\r\n",
            "\r\n",
            "Title: The King James Bible\r\n",
            "\r\n",
            "Release Date: August, 1989 [eBook #10]\r\n",
            "[Most recently updated: December 20, 2021]\r\n",
            "\r\n",
            "Language: English\r\n",
            "\r\n",
            "\r\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK THE KING JAMES BIBLE ***\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "The Old Testament of the King James Version of the Bible\r\n",
            "The First Book of Moses: Called Genesis\r\n",
            "The Second Book of Moses: Called Exodus\r\n",
            "The Third Book of Moses: Called Leviticus\r\n",
            "The Fourth Book of Moses: Called Numbers\r\n",
            "The Fifth Book of Moses: Called Deuteronomy\r\n",
            "The Boo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wne-L-DNHZTP"
      },
      "source": [
        "### 11.1.1. Direct search in a document\n",
        "\n",
        "NLTK Book chapter 3によれば、Regular Expressionを用いて直接的にString Matchingを行うことが出来る。  \n",
        "Pythonに含まれるRegular Expressionに対応するMethodであるfind()やrfind()を用いると、直接Raw Textから特定の語やPatternを見つけることが出来る。 \n",
        "\n",
        "According to NLTK Book chapter 3, String Matching can be performed directly using Regular Expression.  \n",
        "If you use find() or rfind() which are methods corresponding to regular expression included in Python, you can find specific words and patterns directly from raw Text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kND7dTfHZTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b0a1c3-edc0-4264-b7c5-ca0695cfb168"
      },
      "source": [
        "# 原始的な検索: findやrfindを用いると可能。取得した場所から100語を抽出\n",
        "a = raw.find(\"Jesus Christ\")\n",
        "print(a)\n",
        "print(raw.rfind(\"Jesus Christ\"))\n",
        "print(raw[a:a+100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3413560\n",
            "4435139\n",
            "Jesus Christ, the son of David, the\r\n",
            "son of Abraham.\r\n",
            "\r\n",
            "1:2 Abraham begat Isaac; and Isaac begat Jac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpmHQMA_HZTQ"
      },
      "source": [
        "### 11.1.2. Tokenize\n",
        "\n",
        "ここでは、章・行・単語の分割を行う。また、どの行が章に、どの単語がどの行に含まれているのかを記述する。  \n",
        "In this section, we try to divide into chapters, lines, and words from string sequence. We also extract the information which lines are included in the chapter and which words are included in which lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dO1DnxnHZTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432a8128-8ad0-48fe-eca6-8bfcf7351c2e"
      },
      "source": [
        "#from __future__ import division, print_function # Python 2 users only\n",
        "import nltk, re, pprint\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 多数改行する部分を捉えて、文としてピリオドを付与する\n",
        "# Insert period as sentence to find line feed continuity. \n",
        "raw2 = re.sub(r'\\r\\n\\r\\n(\\r\\n)+', \".\\r\\n\", raw)\n",
        "raw2 = re.sub(r'\\.\\.', \".\", raw2) # 上記作業でできた2重ピリオドを削除 delete double period\n",
        "\n",
        "# 入力したものを、文や単語ごとにトークン化\n",
        "# Tokenize processed string\n",
        "sentences = sent_tokenize(raw2)\n",
        "tokens = word_tokenize(raw2)\n",
        "\n",
        "print(\"Num. of Sentences: \" + str(len(sentences)))\n",
        "print(\"Num. of Words: \" + str(len(tokens)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Num. of Sentences: 30002\n",
            "Num. of Words: 952530\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoe46yNOHZTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04327554-e219-4887-bc1c-86f7ef5f369e"
      },
      "source": [
        "# 20行分を表示。Print 20 lines\n",
        "for i in range(101,120):\n",
        "    print(sentences[i])\n",
        "    print(\"-----\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4:11 And now art thou cursed from the earth, which hath opened her\r\n",
            "mouth to receive thy brother’s blood from thy hand; 4:12 When thou\r\n",
            "tillest the ground, it shall not henceforth yield unto thee her\r\n",
            "strength; a fugitive and a vagabond shalt thou be in the earth.\n",
            "-----\n",
            "4:13 And Cain said unto the LORD, My punishment is greater than I can\r\n",
            "bear.\n",
            "-----\n",
            "4:14 Behold, thou hast driven me out this day from the face of the\r\n",
            "earth; and from thy face shall I be hid; and I shall be a fugitive and\r\n",
            "a vagabond in the earth; and it shall come to pass, that every one\r\n",
            "that findeth me shall slay me.\n",
            "-----\n",
            "4:15 And the LORD said unto him, Therefore whosoever slayeth Cain,\r\n",
            "vengeance shall be taken on him sevenfold.\n",
            "-----\n",
            "And the LORD set a mark\r\n",
            "upon Cain, lest any finding him should kill him.\n",
            "-----\n",
            "4:16 And Cain went out from the presence of the LORD, and dwelt in the\r\n",
            "land of Nod, on the east of Eden.\n",
            "-----\n",
            "4:17 And Cain knew his wife; and she conceived, and bare Enoch: and he\r\n",
            "builded a city, and called the name of the city, after the name of his\r\n",
            "son, Enoch.\n",
            "-----\n",
            "4:18 And unto Enoch was born Irad: and Irad begat Mehujael: and\r\n",
            "Mehujael begat Methusael: and Methusael begat Lamech.\n",
            "-----\n",
            "4:19 And Lamech took unto him two wives: the name of the one was Adah,\r\n",
            "and the name of the other Zillah.\n",
            "-----\n",
            "4:20 And Adah bare Jabal: he was the father of such as dwell in tents,\r\n",
            "and of such as have cattle.\n",
            "-----\n",
            "4:21 And his brother’s name was Jubal: he was the father of all such\r\n",
            "as handle the harp and organ.\n",
            "-----\n",
            "4:22 And Zillah, she also bare Tubalcain, an instructer of every\r\n",
            "artificer in brass and iron: and the sister of Tubalcain was Naamah.\n",
            "-----\n",
            "4:23 And Lamech said unto his wives, Adah and Zillah, Hear my voice;\r\n",
            "ye wives of Lamech, hearken unto my speech: for I have slain a man to\r\n",
            "my wounding, and a young man to my hurt.\n",
            "-----\n",
            "4:24 If Cain shall be avenged sevenfold, truly Lamech seventy and\r\n",
            "sevenfold.\n",
            "-----\n",
            "4:25 And Adam knew his wife again; and she bare a son, and called his\r\n",
            "name Seth: For God, said she, hath appointed me another seed instead\r\n",
            "of Abel, whom Cain slew.\n",
            "-----\n",
            "4:26 And to Seth, to him also there was born a son; and he called his\r\n",
            "name Enos: then began men to call upon the name of the LORD.\n",
            "-----\n",
            "5:1 This is the book of the generations of Adam.\n",
            "-----\n",
            "In the day that God\r\n",
            "created man, in the likeness of God made he him; 5:2 Male and female\r\n",
            "created he them; and blessed them, and called their name Adam, in the\r\n",
            "day when they were created.\n",
            "-----\n",
            "5:3 And Adam lived an hundred and thirty years, and begat a son in his\r\n",
            "own likeness, and after his image; and called his name Seth: 5:4 And\r\n",
            "the days of Adam after he had begotten Seth were eight hundred years:\r\n",
            "and he begat sons and daughters: 5:5 And all the days that Adam lived\r\n",
            "were nine hundred and thirty years: and he died.\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGTAYi1DHZTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d56abe3b-28f2-4e67-9095-7a855e3902da"
      },
      "source": [
        "# KJVの内容の最後には、下記のようなパターンがあるので、これを種にして、最終行を得る。\n",
        "# Extract last line using the code of end pattern in Gutenberg Corpus.\n",
        "finreg = re.compile(r'END OF THE PROJECT')\n",
        "endline = [(s1-1, sentences[s1-1]) for s1, s2 in enumerate(sentences) if finreg.search(s2)]\n",
        "print(endline)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(29886, 'Amen.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGpQM9sTHZTR"
      },
      "source": [
        "実際の文書をセグメント化しようとする場合、文書の事前知識を用いるケースが多い。  \n",
        "In you want to segment real documents, we use pre knowledge of this document in many case.\n",
        "\n",
        "聖書では、最初の節は『1章1節』であるので、この場合では 1:1 というパターンが出る。  \n",
        "これを捉えて、タイトル行の行番号を得る。\n",
        "In the Bible-KJV dataset has the pattern '^\\\\:1 ' after the title line. We can use this information to extract book title."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMGBGS4OHZTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5c599e-00d5-41aa-e75a-5c7ab7ecfbc8"
      },
      "source": [
        "#書物の情報の抽出: 聖書は各章ほぼ必ず1:1から始まるので、マッチングした行の1つ上を抽出\n",
        "# Extracting Book Information: The Bible almost always starts at 1: 1 so extract one line above the matched line\n",
        "regexp = re.compile(r'^1\\:1 ')\n",
        "finreg = re.compile(r'^End of the Project')\n",
        "titlelist = [(s1-1, sentences[s1-1]) for s1, s2 in enumerate(sentences) if regexp.search(s2)]\n",
        "print(len(titlelist))\n",
        "for t in titlelist:\n",
        "    print(t)\n",
        "    print('-----')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66\n",
            "(7, 'The First Book of Moses:  Called Genesis.')\n",
            "-----\n",
            "(1475, 'The Second Book of Moses:  Called Exodus.')\n",
            "-----\n",
            "(2587, 'The Third Book of Moses:  Called Leviticus.')\n",
            "-----\n",
            "(3252, 'The Fourth Book of Moses:  Called Numbers.')\n",
            "-----\n",
            "(4244, 'The Fifth Book of Moses:  Called Deuteronomy.')\n",
            "-----\n",
            "(4975, 'The Book of Joshua.')\n",
            "-----\n",
            "(5468, 'The Book of Judges.')\n",
            "-----\n",
            "(6176, 'The Book of Ruth.')\n",
            "-----\n",
            "(6280, 'The First Book of Samuel\\r\\n\\r\\nOtherwise Called:\\r\\n\\r\\nThe First Book of the Kings.')\n",
            "-----\n",
            "(7294, 'The Second Book of Samuel\\r\\n\\r\\nOtherwise Called:\\r\\n\\r\\nThe Second Book of the Kings.')\n",
            "-----\n",
            "(8133, 'The First Book of the Kings\\r\\n\\r\\nCommonly Called:\\r\\n\\r\\nThe Third Book of the Kings.')\n",
            "-----\n",
            "(8961, 'The Second Book of the Kings\\r\\n\\r\\nCommonly Called:\\r\\n\\r\\nThe Fourth Book of the Kings.')\n",
            "-----\n",
            "(9850, 'The First Book of the Chronicles.')\n",
            "-----\n",
            "(10616, 'The Second Book of the Chronicles.')\n",
            "-----\n",
            "(11444, 'Ezra.')\n",
            "-----\n",
            "(11672, 'The Book of Nehemiah.')\n",
            "-----\n",
            "(12027, 'The Book of Esther.')\n",
            "-----\n",
            "(12198, 'The Book of Job.')\n",
            "-----\n",
            "(13273, 'The Book of Psalms.')\n",
            "-----\n",
            "(15726, 'The Proverbs.')\n",
            "-----\n",
            "(16565, 'Ecclesiastes\\r\\n\\r\\nor\\r\\n\\r\\nThe Preacher.')\n",
            "-----\n",
            "(16782, 'The Song of Solomon.')\n",
            "-----\n",
            "(16912, 'The Book of the Prophet Isaiah.')\n",
            "-----\n",
            "(18232, 'The Book of the Prophet Jeremiah.')\n",
            "-----\n",
            "(19557, 'The Lamentations of Jeremiah.')\n",
            "-----\n",
            "(19718, 'The Book of the Prophet Ezekiel.')\n",
            "-----\n",
            "(20878, 'The Book of Daniel.')\n",
            "-----\n",
            "(21219, 'Hosea.')\n",
            "-----\n",
            "(21428, 'Joel.')\n",
            "-----\n",
            "(21494, 'Amos.')\n",
            "-----\n",
            "(21636, 'Obadiah.')\n",
            "-----\n",
            "(21661, 'Jonah.')\n",
            "-----\n",
            "(21720, 'Micah.')\n",
            "-----\n",
            "(21840, 'Nahum.')\n",
            "-----\n",
            "(21895, 'Habakkuk.')\n",
            "-----\n",
            "(21965, 'Zephaniah.')\n",
            "-----\n",
            "(22017, 'Haggai.')\n",
            "-----\n",
            "(22052, 'Zechariah.')\n",
            "-----\n",
            "(22262, 'Malachi.')\n",
            "-----\n",
            "(22349, 'The Gospel According to Saint Matthew.')\n",
            "-----\n",
            "(23393, 'The Gospel According to Saint Mark.')\n",
            "-----\n",
            "(24082, 'The Gospel According to Saint Luke.')\n",
            "-----\n",
            "(25211, 'The Gospel According to Saint John.')\n",
            "-----\n",
            "(26180, 'The Acts of the Apostles.')\n",
            "-----\n",
            "(27080, 'The Epistle of Paul the Apostle to the Romans.')\n",
            "-----\n",
            "(27514, 'The First Epistle of Paul the Apostle to the Corinthians.')\n",
            "-----\n",
            "(27980, 'The Second Epistle of Paul the Apostle to the Corinthians.')\n",
            "-----\n",
            "(28206, 'The Epistle of Paul the Apostle to the Galatians.')\n",
            "-----\n",
            "(28345, 'The Epistle of Paul the Apostle to the Ephesians.')\n",
            "-----\n",
            "(28418, 'The Epistle of Paul the Apostle to the Philippians.')\n",
            "-----\n",
            "(28498, 'The Epistle of Paul the Apostle to the Colossians.')\n",
            "-----\n",
            "(28553, 'The First Epistle of Paul the Apostle to the Thessalonians.')\n",
            "-----\n",
            "(28621, 'The Second Epistle of Paul the Apostle to the Thessalonians.')\n",
            "-----\n",
            "(28652, 'The First Epistle of Paul the Apostle to Timothy.')\n",
            "-----\n",
            "(28740, 'The Second Epistle of Paul the Apostle to Timothy.')\n",
            "-----\n",
            "(28805, 'The Epistle of Paul the Apostle to Titus.')\n",
            "-----\n",
            "(28837, 'The Epistle of Paul the Apostle to Philemon.')\n",
            "-----\n",
            "(28852, 'The Epistle of Paul the Apostle to the Hebrews.')\n",
            "-----\n",
            "(29082, 'The General Epistle of James.')\n",
            "-----\n",
            "(29204, 'The First Epistle General of Peter.')\n",
            "-----\n",
            "(29276, 'The Second General Epistle of Peter.')\n",
            "-----\n",
            "(29318, 'The First Epistle General of John.')\n",
            "-----\n",
            "(29439, 'The Second Epistle General of John.')\n",
            "-----\n",
            "(29455, 'The Third Epistle General of John.')\n",
            "-----\n",
            "(29471, 'The General Epistle of Jude.')\n",
            "-----\n",
            "(29492, 'The Revelation of Saint John the Divine.')\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikwrDbTmHZTR"
      },
      "source": [
        "# 書物内のデータを再分割。行分割を行っていたので、上記の書物情報の行番号を頼りに、\n",
        "# 書物のタイトルを除いた本文を一つのリストにしている。\n",
        "# Subdivide the data in the book. \n",
        "# Because the line is divided, the text excluding the title of the book is made into one list, \n",
        "# relying on the line number of the book information.\n",
        "books = []\n",
        "prev = 0\n",
        "for i, title in enumerate(titlelist):\n",
        "    book = []\n",
        "    if i == 0:\n",
        "        prev = title[0]\n",
        "        continue\n",
        "    for j in range(title[0]-prev-1):\n",
        "        book += word_tokenize(sentences[prev+j+1])\n",
        "    books.append(book)\n",
        "    prev = title[0]\n",
        "book = []\n",
        "for i in range(endline[0][0]-prev):\n",
        "    book += word_tokenize(sentences[title[0]+i+1])\n",
        "books.append(book)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n32HZjvSHZTS"
      },
      "source": [
        "節の分離: 行内に入り込んでしまっているケースが多いので、一単語ずつ丁寧にSearchする必要がある。  \n",
        "Separation of clauses: Patterns indicating clauses buried in the line sometime, so it is necessary to find one by one carefully in each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSjTYYqIHZTS"
      },
      "source": [
        "# 節に関してはちゃんと節毎に分かれていない。\n",
        "bookplace = []\n",
        "verses = []\n",
        "verse = []\n",
        "place = 0\n",
        "re_sever = re.compile(r'^[1-9][0-9]*\\:[1-9][0-9]*')\n",
        "for j,book in enumerate(books):\n",
        "    bookplace.append(place)\n",
        "    for i, s in enumerate(book):\n",
        "        if re_sever.search(s):\n",
        "            if verse != []:\n",
        "                verses.append(verse)\n",
        "                place += 1\n",
        "            verse = []\n",
        "            verse.append(s)\n",
        "        else:\n",
        "            verse.append(s)\n",
        "    verses.append(verse)\n",
        "    place += 1\n",
        "    verse = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0SqdMlXHZTS"
      },
      "source": [
        "# 節数の確認, Check the number of verses, but this is less than true.\n",
        "print(len(verses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVR5MYKTHZTS"
      },
      "source": [
        "# Versesの中の書物の先頭の場所, Check the number of position which is indicates start of each verse\n",
        "print(bookplace)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLRiiUXHHZTT"
      },
      "source": [
        "### 11.1.3. Stemming, Lemmatizing\n",
        "\n",
        "(★ Assignment Remark):\n",
        "\n",
        "検索のための単語をそろえるために、StemmingやLemmatizingを行う。  \n",
        "To standardize words for search, we shuold do stemming and lemmatizing.   \n",
        "\n",
        " - Stemmingは基本的に語の時制等『接頭語・接尾語』などを削るために行う。  \n",
        " Stemming is able to eliminate prefix and postfix to make standardization of a meaning of words.\n",
        " - LemmatizingはStemmingで処理出来ない変化形の語ををWordnetなどの辞書を用いて正規化するために行う。  \n",
        " Lemmatizing is able to trace the changing of words such as eat to ate which is not able to transform by Stemming.\n",
        "\n",
        "ここでは、Porter Stemmerを用いたStemmingの例として、Chapter 3.3で行ったIndexTextクラスを用いる。さらに、Lemmatizerを用いたLemmatizingの例として、IndexTextクラスを修正したLemmatizedTextクラスを作成する。  \n",
        "Here, we construct IndexText class which utilize Porter stemmer for stemming, and LemmatizedText class which utilize Wordnet lemmatizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifptcTBtHZTT"
      },
      "source": [
        "# Text Normalization: Ch.3で行ったクラスを利用\n",
        "class IndexedText(object):\n",
        "\n",
        "    def __init__(self, stemmer, text):\n",
        "        self._text = text\n",
        "        self._stemmer = stemmer\n",
        "        self._index = nltk.Index((self._stem(word), i)\n",
        "                                 for (i, word) in enumerate(text))\n",
        "\n",
        "    def concordance(self, word, width=40):\n",
        "        key = self._stem(word)\n",
        "        wc = int(width/4)                # words of context\n",
        "        for i in self._index[key]:\n",
        "            lcontext = ' '.join(self._text[i-wc:i])\n",
        "            rcontext = ' '.join(self._text[i:i+wc])\n",
        "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
        "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
        "            print(ldisplay, rdisplay)\n",
        "\n",
        "    def _stem(self, word):\n",
        "        return self._stemmer.stem(word).lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-NCfYhZHZTT"
      },
      "source": [
        "# Stemming: Porter Stemmerを利用\n",
        "porter = nltk.PorterStemmer()\n",
        "text = IndexedText(porter, tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwFm68AtHZTT"
      },
      "source": [
        "# Concordanceの利用: 一種のSearch Engineと言える。\n",
        "text.concordance('say')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxiavnveHZTU"
      },
      "source": [
        "# WordNetを用いたLemmatizerの例 (Chapter 3.4.)\n",
        "nltk.download('wordnet')\n",
        "wnl = nltk.WordNetLemmatizer()\n",
        "tkn = [wnl.lemmatize(t) for t in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaqDM6qhHZTU"
      },
      "source": [
        "# Stemmer Indexの改善: Wordnet Lemmatizerを用いた場合\n",
        "class LemmatizedText(object):\n",
        "\n",
        "    def __init__(self, lemmatizer, text):\n",
        "        self._text = text\n",
        "        self._lemmatizer = lemmatizer\n",
        "        self._index = nltk.Index((self._lemmatize(word), i)\n",
        "                                 for (i, word) in enumerate(text))\n",
        "\n",
        "    def concordance(self, word, width=40):\n",
        "        key = self._lemmatize(word)\n",
        "        wc = int(width/4)                # words of context\n",
        "        for i in self._index[key]:\n",
        "            lcontext = ' '.join(self._text[i-wc:i])\n",
        "            rcontext = ' '.join(self._text[i:i+wc])\n",
        "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
        "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
        "            print(ldisplay, rdisplay)\n",
        "\n",
        "    def _lemmatize(self, word):\n",
        "        return self._lemmatizer.lemmatize(word).lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGVgeH1DHZTU"
      },
      "source": [
        "# Lemmatizing: Wordnet Lemmatizerを利用\n",
        "text2 = LemmatizedText(wnl, tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxSoue8cHZTU"
      },
      "source": [
        "*Index化の結果*\n",
        "Result of indexing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6beo8BwHZTV"
      },
      "source": [
        "print(list( (v, text2._index[v]) for i,v in enumerate(text2._index) if i > 140 and i < 145 ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY8aX4cSHZTV"
      },
      "source": [
        "# Concordanceの利用: Stemmingと違い、Sayingのサポートがない\n",
        "text2.concordance('say')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucw5IXb7HZTV"
      },
      "source": [
        "Lemmatizerでは、Stemmingのようなことは成されないが、一方で、名詞が壊れる等の問題は少ない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOYHnB9hHZTW"
      },
      "source": [
        "# Stemming, Lemmatizingを行った後、単語リストとして使用するためのリスト化操作\n",
        "keys = text2._index.keys()\n",
        "# Python3は以下も行う\n",
        "keys = [*keys]\n",
        "print(keys[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aG8iUVmHZTW"
      },
      "source": [
        "### 11.1.4 POS Tagging and Extract Feature\n",
        "\n",
        "(★ Assignment Remark)\n",
        "\n",
        "Lecture 6において、Featureを取り出すには、有用なFeatureを考察する必要がある。  \n",
        "From the lecture 6, if we want to extract feature from documents, we should consider what is the effective feature.\n",
        "\n",
        "特に、検索では、全文を検索するのは非常に時間が掛かるために、限られた特徴量に絞って検索を行うことが多い。  \n",
        "Especiall in a search, we often perform a search with a limited amount of features since it takes a very long time to search the entire text.\n",
        "\n",
        "本パートでは、POSタグ付けを行い、名詞、動詞、形容詞のうち、全体的に出現頻度の高い語を特徴量として扱う。  \n",
        "In this part, we perform POS tagging on words, and treat words with high overall appearance frequency as features of nouns, verbs, and adjectives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JhaSbBdHZTW"
      },
      "source": [
        "#Feature Vectorの構築: WordlistからStopwordsを省く。基本的に名詞や動詞、形容詞を利用。\n",
        "# NLTK内部に入っているPOS TAGを利用。文脈を見ているわけではないのであまり精度は良くない\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "wordtags = nltk.pos_tag(keys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1utXG3sHZTX"
      },
      "source": [
        "nltk.FreqDist(tag for (word, tag) in wordtags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjmu6omUHZTX"
      },
      "source": [
        "tkns = nltk.FreqDist(word for word in tkn)\n",
        "tknlst = tkns.most_common(5000)\n",
        "feat = list(w.lower() for (w,c) in tknlst)\n",
        "# for listing names\n",
        "feat2 = list(w for w,c in tknlst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPVRWac_HZTX"
      },
      "source": [
        "# Stopwordを省いた上で、今回は名詞と動詞のみで構成する要素を抽出\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopWords = set(stopwords.words('english'))\n",
        "features = [(word , tag) for (word , tag) in wordtags if word not in stopWords and word in feat \n",
        "            if tag == 'NN' or tag == 'NNP' or tag =='PP' or tag == 'PRP' or tag =='VBD' or tag == 'VBG'\n",
        "            or tag == 'VBP' or tag == 'VBN' or tag == 'JJ' or tag == 'JJS' or tag == 'JJR']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvm7E-pOHZTX"
      },
      "source": [
        "nltk.FreqDist(tag for (word, tag) in features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgWBwjlWHZTY"
      },
      "source": [
        "# [6]で書物情報を抽出しているので、書物毎に出てくる単語をLemmatizeしておく。\n",
        "# 各書物毎のLematizeされた単語リストを形成\n",
        "lemmabooks = []\n",
        "for book in books:\n",
        "    lemmabook = []\n",
        "    for w in book:\n",
        "        lemmabook.append(wnl.lemmatize(w).lower())\n",
        "    lemmabooks.append(lemmabook)\n",
        "collection = nltk.TextCollection(lemmabooks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IyGRtLZHZTY"
      },
      "source": [
        "# 同様に、節の区切りもLemmatizeしておくことにする。\n",
        "lemmaverses = []\n",
        "for verse in verses:\n",
        "    lemmaverse = []\n",
        "    for w in verse:\n",
        "        lemmaverse.append(wnl.lemmatize(w).lower())\n",
        "    lemmaverses.append(lemmaverse)\n",
        "collection2 = nltk.TextCollection(lemmaverses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79qi1wUTHZTY"
      },
      "source": [
        "# 特徴的な語彙をそろえるために、出現上位3000語の単語のうち、featureで指定した名詞・動詞・形容詞の単語リストを作成\n",
        "flist = list ( v.lower() for (v,k) in features)\n",
        "flist.sort()\n",
        "print(flist[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suQAqy8ZHZTY"
      },
      "source": [
        "# For listing name and place\n",
        "tmplist = list( v for v in feat2 if v[0].isupper())\n",
        "nlist = list( v for v in tmplist if v.lower() not in feat2 and (v.lower(),'NN') in features or (v.lower(), 'NNP') in features)\n",
        "print(len(nlist))\n",
        "print(nlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViJXvawyHZTY"
      },
      "source": [
        "# Exercise Attendance\n",
        "\n",
        "Please make concordance of 3 person's name which select from in previous list (which includes some noise word).  \n",
        "Also, please find a person's name which has less than 10 times appear in the Bible.\n",
        "\n",
        "Hint: text.concordance('David')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uyT5figHZTZ"
      },
      "source": [
        "## 11.2. Feature Vector Construction\n",
        "\n",
        "検索対象となる特徴ベクトルやIndex化を行う。  \n",
        "In this section, we make index of feature vector.\n",
        "\n",
        "ここでのIndexは、\n",
        "- 単語頻度リスト\n",
        "- TF・IDFベクトル\n",
        "\n",
        "である。\n",
        "\n",
        "In here, the index data is:\n",
        " - word count list\n",
        " - TF\\*IDF vector\n",
        " \n",
        "ここでのTF\\*IDFは、単純な掛け算によるTF\\*IDFで、対数TF\\*IDFではないです。\n",
        "This TF\\*IDF is naive multiplication not log-scale TF\\*IDF.\n",
        "\n",
        "なお、データサイズが大きいので、この部分は非常に時間が掛かるので注意してください。  \n",
        "Please notice about this process needs to take so much time by datasize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weB7vTrtHZTZ"
      },
      "source": [
        "# TFIDFの計算。各文書毎に計算を行う。なお、collectionで計算しているので、Featureのみではなく全単語で計算している。\n",
        "verselist = []\n",
        "for i, verse in enumerate(lemmaverses):\n",
        "    fv = []\n",
        "    wc = []\n",
        "    for term in flist:\n",
        "        fv.append(collection2.tf_idf(term, verse))\n",
        "        wc.append(verse.count(term))\n",
        "    verselist.append((i, verse[0], fv, wc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGE24RluHZTZ"
      },
      "source": [
        "chaplist = []\n",
        "for i, doc in enumerate(lemmabooks):\n",
        "    fv = []\n",
        "    wc = []\n",
        "    for term in flist:\n",
        "        fv.append(collection.tf_idf(term, doc))\n",
        "        wc.append(doc.count(term))\n",
        "    chaplist.append((i, titlelist[i][1], fv, wc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA3krkOEHZTZ"
      },
      "source": [
        "# 『創世記 1;1』のTFIDFベクトル\n",
        "print(verselist[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc7LWpw9HZTZ"
      },
      "source": [
        "# 『創世記』のTFIDFベクトル\n",
        "print(chaplist[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z6vR-xlHZTZ"
      },
      "source": [
        "# 『ヨハネの黙示録』のTFIDFベクトル\n",
        "print(chaplist[65])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxQgSQMiHZTa"
      },
      "source": [
        "## 11.3. Retrieval\n",
        "(★ Assignment Remark)  \n",
        "Index化が終われば、Indexを走査することで、情報を検索することができる。  \n",
        "After indexing, you can retrieve relative verse with your query.\n",
        "\n",
        "幾つかの検索モデルで試してみましょう  \n",
        "Please check some retrieval model\n",
        " - Boolean Model\n",
        " - Extended Boolean Model (Fuzzy Model)\n",
        " - Vector Space Model\n",
        "\n",
        "Query List:\n",
        "- 検索: 『 Is Jesus Christ the son of GOD? 』\n",
        "- 検索; 『 Can Jesus Christ serves us from our sin by his cross and his grace?』"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi6toTwTHZTa"
      },
      "source": [
        "# Extended Boolean Model: 単語包含数のMAX検索\n",
        "def extboolsearch(query, datas, datalist, taglist, lemmatizer):\n",
        "    tokens = word_tokenize(query)\n",
        "    lemmas = list( lemmatizer.lemmatize(w.lower()) for w in tokens)\n",
        "    points = []\n",
        "    for i, v in enumerate(datalist):\n",
        "        k = 0\n",
        "        for word in set(lemmas):\n",
        "            if word in flist:\n",
        "                k = max(k, v[2][taglist.index(word)])\n",
        "        points.append((k,i))\n",
        "    points.sort(reverse=True)\n",
        "    for k, i in points[:10]:\n",
        "        print((k,datas[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWK47rjzHZTa"
      },
      "source": [
        "extboolsearch(query1, verses, verselist, flist, wnl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW946bSAHZTb"
      },
      "source": [
        "extboolsearch(query2, verses, verselist, flist, wnl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7P5qeIAzrSaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9QZdvJTHZTb"
      },
      "source": [
        "# Vector Space Model with Cosine Collelation: TF・IDFとQueryの生起確率との内積\n",
        "def cosinesearch(query, datas, datalist, taglist, lemmatizer, nums):\n",
        "    tokens = word_tokenize(query)\n",
        "    lemmas = list( lemmatizer.lemmatize(w.lower()) for w in tokens if w.lower() in flist)\n",
        "    setlem = set(lemmas)\n",
        "    points = []\n",
        "    lenlem = 0\n",
        "    for s in setlem:\n",
        "        lenlem = lemmas.count(s)\n",
        "    for i, v in enumerate(datalist):\n",
        "        lenv = 0\n",
        "        for vt in v[2]:\n",
        "            lenv += vt**2\n",
        "        k = 0\n",
        "        for word in set(lemmas):\n",
        "            if word in flist:\n",
        "                k += 2 *  v[2][taglist.index(word)]*lemmas.count(word)\n",
        "        k = k / (lenlem + lenv)\n",
        "        points.append((k,i))\n",
        "    points.sort(reverse=True)\n",
        "    for k, i in points[:nums]:\n",
        "        print((k, datas[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCc_-7UyHZTb"
      },
      "source": [
        "cosinesearch(query1, verses, verselist, flist, wnl, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN3oIVaXHZTb"
      },
      "source": [
        "cosinesearch(query2, verses, verselist, flist, wnl,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV0JdCn6HZTb"
      },
      "source": [
        "cosinesearch(query1, books, chaplist, flist, wnl,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7OnN7JqHZTb"
      },
      "source": [
        "cosinesearch(query2, books, chaplist, flist, wnl,1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}