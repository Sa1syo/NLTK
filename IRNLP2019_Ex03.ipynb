{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sa1syo/NLTK/blob/main/IRNLP2019_Ex03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1pWrK7IOgmP"
      },
      "source": [
        "# Exercise 3. Processing Raw Text (3.1, 2-6, 9)\n",
        "\n",
        "For International Students: goto http://www.nltk.org/book/ch03.html  \n",
        "Almost corresponded about:\n",
        " - Lesson 1: 3.1\n",
        " - Lesson 2: 3.3\n",
        " - Lesson 3: 3.4\n",
        " - Lesson 4: 3.5\n",
        " - Lesson 5: 3.6\n",
        " - Lesson 6: 3.9\n",
        " \n",
        "(★ Assignment Remark): Please read carefully about 3.1 and 3.3.\n",
        "\n",
        "この演習では、Webや平文のデータをどうやって読み込み、言語資源として利用するかを学ぶ。  \n",
        "また、言語リソースとして扱うために必要な処理として、\n",
        "\n",
        " - トークン化\n",
        " - ステミング (Stemming)/ 見出し語化 (Lemmatizing)\n",
        " - セグメンテーション\n",
        "\n",
        "等のことをNLTKでどのように行うかを学ぶ。\n",
        "\n",
        "※ 以下の環境では、特にしていなければ以下のimport文から開始された対話セッション, あるいはコードであると仮定して進めることとする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EO8t33jOgmQ",
        "outputId": "927dcee5-48d1-46a8-a399-424a1d5d543d"
      },
      "source": [
        "#from __future__ import division  # Python 2 users only\n",
        "import nltk, re, pprint\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxv8nYPFOgmR"
      },
      "source": [
        "## Lesson 1. Accessing Text from the Web and from Disk (3.1.)\n",
        "\n",
        "### 1.1. Electronic Book\n",
        "\n",
        "電子書籍 (Project Gutenberg) からの参照をWeb経由で行うために。\n",
        "- http://www.gutenberg.org/catalog/\n",
        "- 25,000件の無料オンライン書籍のカタログ\n",
        "- 50以上の言語\n",
        "- text[2554] : an English translation of Crime and Punishment (罪と罰, ドスドエフスキー）\n",
        "\n",
        "なお、Project Gutenbergのテキストを読み込もうとすると、過去のNLTK Bookの中にあるWebページではアクセスできなくなっている（2554-0.txt）ことから、URLを見直す必要がある。  \n",
        "また、Python 3から、UrlLibは、urlopen()メソッドがrequest内に配置されているため、そのように変更する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz2vVQcXOgmR",
        "outputId": "7738f427-a0b6-4e0c-c412-49cb00c1ecf2"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "print('type:', type(raw), ' length:', len(raw))\n",
        "print(raw[:200])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type: <class 'str'>  length: 1176812\n",
            "﻿The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWQ5L96nOgmU"
      },
      "source": [
        "言語処理を行うために、文字列を単語と句読点に分解する。この処理をトークン化 (Tokenization)と呼ぶ。\n",
        "- 文字列を単語と句読点に分割。\n",
        "- 空白や改行、空白行を除去"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a92dNI-MOgmX",
        "outputId": "80ae04d7-5e28-4311-ed8f-a77bbb2532ed"
      },
      "source": [
        "tokens = word_tokenize(raw)\n",
        "print('type:', type(tokens), ' length:', len(tokens))\n",
        "print(tokens[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type: <class 'list'>  length: 257727\n",
            "['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by', 'Fyodor', 'Dostoevsky', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Uot597iOgmY"
      },
      "source": [
        "トークン化された情報を利用すれば、Ex. 1で紹介したSlicing等の言語処理を行うことが出来る。\n",
        "- nltk.Textを用いてトークン化された情報をリスト化する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbT-rN1hOgmY",
        "outputId": "91e93d5c-54ad-49a7-d23c-94eabe0281dd"
      },
      "source": [
        "text = nltk.text.Text(tokens)\n",
        "print('type:', type(text))\n",
        "print(text[1024:1062])\n",
        "print(text.collocations())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type: <class 'nltk.text.Text'>\n",
            "['an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.', 'bridge', '.', 'He', 'had', 'successfully']\n",
            "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
            "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
            "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
            "great deal; young man; Nikodim Fomitch; Ilya Petrovitch; Project\n",
            "Gutenberg; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
            "heavens\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMbXgnG1OgmZ"
      },
      "source": [
        "※ collocations() is buggy. Please use collocation_list() to get word collocation list in Python 3.  \n",
        "collocations()にはバグが含まれている可能性がありますので、使用する場合はcollocation_list()を利用してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxPoMwQ4OgmZ"
      },
      "source": [
        "語彙のリスト化が行えると、その文書から、単語ごとの検索を行うことが出来る。  \n",
        "（fund, rfindを利用する)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc0jk2ByOgmZ",
        "outputId": "8d3d88c5-35f4-4d7f-c453-8a8730d0577d"
      },
      "source": [
        "print('position of \"PART IV\":', raw.find(\"PART IV\"))\n",
        "print('reverse position of \"End of Project Gutenberg’s Crime\":', raw.rfind(\"End of Project Gutenberg’s Crime\"))\n",
        "a = raw.find(\"PART IV\")\n",
        "raw2 = raw[a:]\n",
        "print('position of \"PART IV\" (cutted out):', raw2.find(\"PART IV\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "position of \"PART IV\": 595833\n",
            "reverse position of \"End of Project Gutenberg’s Crime\": 1157812\n",
            "position of \"PART IV\" (cutted out): 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7_rcRuzOgma"
      },
      "source": [
        "### 1.2. Processing HTML\n",
        "\n",
        "(★ Assignment Remark) Web上のドキュメントの殆どは、HTML形式で保存されているので、そうしたWeb上のテキストを収集し保存しておけば、一度アクセスするだけでその後は何度も閲覧できる。（キャッシュと呼んでいる。）  \n",
        "\n",
        "HTMLを扱う場合: urlopenを利用してデータを取得し、保存しておく。  \n",
        "対象とするドキュメント: BBC \"Blondes to die out in 200 years\" (ブロンドは200年後に死に絶える) = 都市伝説"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "0ytGiXxIOgma",
        "outputId": "c625142b-c982-4d32-cb8f-a0637a47c837"
      },
      "source": [
        "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
        "html = request.urlopen(url).read().decode('utf8')\n",
        "html[:200]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<head>\\r\\n<title>BBC NEWS | Health | Blondes \\'to die out in 200 years\\'</title>\\r\\n<meta '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J02uf8OhOgma"
      },
      "source": [
        "HTMLには『タグ』が含まれている (大抵、山かっこ '<', '>' で囲まれている）。  \n",
        "この『タグ』を除去して、テキストを抽出する必要がある（Parsingと呼んだりする)。\n",
        "HTMLからテキストの抽出では、PythonではBeautifulSoupを利用してタグの除去を行う。\n",
        "\n",
        "※ BeautifulSoupが入ってないかもしれないので、利用できない場合は飛ばしてください。\n",
        "\n",
        "インストール方法 (各自のPC上で）\n",
        " - pip install -U bs4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvcBvzp7Ogma",
        "outputId": "b0ac4feb-95a6-4374-a785-a818256a8519"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "raw = BeautifulSoup(html).get_text()\n",
        "tokens = word_tokenize(raw)\n",
        "print(tokens[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'NEWS', 'SPORT', 'WEATHER', 'WORLD', 'SERVICE', 'A-Z', 'INDEX', 'SEARCH', 'You', 'are', 'in', ':', 'Health', 'News', 'Front', 'Page', 'Africa', 'Americas', 'Asia-Pacific', 'Europe', 'Middle', 'East', 'South', 'Asia', 'UK', 'Business', 'Entertainment', 'Science/Nature', 'Technology', 'Health', 'Medical', 'notes', '--', '--', '--', '--', '--', '--', '-', 'Talking', 'Point', '--', '--', '--', '--', '--', '--', '-', 'Country', 'Profiles', 'In', 'Depth', '--', '--', '--', '--', '--', '--', '-', 'Programmes', '--', '--', '--', '--', '--', '--', '-', 'SERVICES', 'Daily', 'E-mail', 'News', 'Ticker', 'Mobile/PDAs', '--', '--', '--', '--', '--', '--', '-', 'Text', 'Only', 'Feedback', 'Help', 'EDITIONS', 'Change', 'to', 'UK']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1PYJ35wOgmb"
      },
      "source": [
        "内容を見ると、タグを除去しただけでは、ナビゲーション等に使われている文言がそのまま残ってしまうので、本文の部分を抽出するのには、一定の試行錯誤が必要となる場合が多い。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9qb3OtFOgmb",
        "outputId": "65581f01-6a30-4f95-a58a-c57529d72314"
      },
      "source": [
        "tokens = tokens[110:390]\n",
        "text = nltk.Text(tokens)\n",
        "text.concordance('gene')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Displaying 5 of 5 matches:\n",
            "hey say too few people now carry the gene for blondes to last beyond the next \n",
            "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
            " have blonde hair , it must have the gene on both sides of the family in the g\n",
            "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
            "des would disappear is if having the gene was a disadvantage and I do not thin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j-wJkUwOgmc"
      },
      "source": [
        "### 1.3. Processing from the result of search engine\n",
        "検索エンジン\n",
        " - 検索エンジンも一つのテキストの巨大なコーパスとみることができる。\n",
        " - 特に、興味のある言語パターンを見つける可能性が高い\n",
        " - 検索パターンの許容範囲制限があったり、一貫性のない結果によるバイアスが生じる。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ttLE0kjOgmc"
      },
      "source": [
        "### 1.4. RSS Feed\n",
        "RSS Feedを用いる\n",
        "- Universal Feed Parserを使うと、RSS Feedもそのまま読める\n",
        "\n",
        "※ feedparserも入ってないかもしれない。\n",
        "- pip install -U feedparser\n",
        "\n",
        "以下の例は、その日のFeedによってタイトルが変わる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc1gpgIJRLUF",
        "outputId": "78dcf731-10ab-47c2-af2d-19cf01484f9f"
      },
      "source": [
        "!pip install -U feedparser"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=961c8689e86f4bba900daf8e598ffb3f70ea4ed2ed6a05255adafbf71946406d\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/63/2f/117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.10 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeqdRfxbOgmc",
        "outputId": "12f14664-a396-4b6e-9572-1809aa600b1a"
      },
      "source": [
        "import feedparser\n",
        "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
        "print(llog['feed']['title'])\n",
        "print('length:',len(llog.entries))\n",
        "post = llog.entries[2]\n",
        "print('title:', post.title)\n",
        "content = post.content[0].value\n",
        "print('content:', content[:200])\n",
        "# Text book はnltkでやっていますが、既にbs4で行う形に置き換えられています。\n",
        "raw = BeautifulSoup(content).get_text()\n",
        "print(word_tokenize(raw)[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language Log\n",
            "length: 13\n",
            "title: Fake &#039;Asian&#039; speech at commencement\n",
            "content: <p>Here's a different take on \"<a href=\"https://languagelog.ldc.upenn.edu/nll/?p=57361\" rel=\"noopener\" target=\"_blank\">plastic</a>\" Chinese&#8230; Michell Quinn, \"<a href=\"https://www.chicagotribune.c\n",
            "['Here', \"'s\", 'a', 'different', 'take', 'on', '``', 'plastic', \"''\", 'Chinese…', 'Michell', 'Quinn', ',', '``', 'PNW', 'Chancellor', 'Keon', 'apologizes', 'for', '‘', 'offensive', 'and', 'insensitive', '’', 'remark', 'during', 'commencement', \"''\", ',', 'Chicago', 'Tribune', '12/14/2022', ':', 'Purdue', 'University', 'Northwest', '’', 's', 'Chancellor', 'Thomas', 'Keon', 'is', 'apologizing', 'for', 'a', 'culturally', 'insensitive', 'remark', 'he', 'made', 'during', 'the', 'first', 'of', 'two', 'commencement', 'ceremonies', 'Dec.', '10', '.', 'The', 'comment', 'was', 'a', 'response', 'after', 'commencement', 'keynote', 'speaker', 'Jim', 'Dedelow', 'finished', 'his', 'speech', '.', 'Dedelow', 'in', 'his', 'speech', 'talked', 'about', 'a', 'made-up', 'language', 'he', 'created', 'to', 'entertain', 'his', 'new', 'granddaughter', 'and', 'at', 'one', 'point', 'used', 'it', 'to', 'calm', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_LOy0cXOgmd"
      },
      "source": [
        "### 1.5. Reading Local Files\n",
        "\n",
        "ローカルファイルを読み込むには: \n",
        " - pythonのfile openを使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CCeREWAOgmd"
      },
      "source": [
        "f = open('document.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bR4qLXSOgmd"
      },
      "source": [
        "そのまま利用するとエラーが発生するため、先に自分の居るディレクトリにファイルがあるかを確認する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvwumRZPOgmd",
        "outputId": "3fe96ae6-a178-4154-d980-98247a96e7a7"
      },
      "source": [
        "import os\n",
        "print(os.listdir('.')[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', 'document.txt', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMMlJx6YOgme"
      },
      "source": [
        "read()関数: そのまま、ファイル全体をメモリに読み出す。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iH69rJNOgme",
        "outputId": "e3ae8873-0120-492b-87c9-1161d93a3b0a"
      },
      "source": [
        "print(f.read())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time flies like an arrow.\n",
            "Fruit flies like a banana.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jICVDb_Ogme"
      },
      "source": [
        "open()関数を用いてループしながら行ごとに読む"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny9k7QuyOgme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef7041f-0922-4280-82ca-78c527d379db"
      },
      "source": [
        "f = open('document.txt', 'r')\n",
        "for line in f:\n",
        "    print(line.strip())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time flies like an arrow.\n",
            "Fruit flies like a banana.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucqPot0xOgmf"
      },
      "source": [
        "nltk.data.find()を用いてパスを取得する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRiOH-WhOgmf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e32e8f-127b-4bf8-ab5f-6a31e40c13e0"
      },
      "source": [
        "path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')\n",
        "raw = open(path, 'r').read()\n",
        "print(raw[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Moby Dick by Herman Melville 1851]\n",
            "\n",
            "\n",
            "ETYMOLOGY.\n",
            "\n",
            "(Supplied by a Late Consumptive Usher to a Grammar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjj-nq3EOgmf"
      },
      "source": [
        "### 1.6. PDF/ MSWord等のファイルを用いたい場合\n",
        "- 基本的にはできません。（予め、ファイルを開き、テキストとしてローカルドライブに保存する必要がある）\n",
        "- PDF: pip install pdfminer3k\n",
        "- Word: pip install textract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKyU6AGhOgmf"
      },
      "source": [
        "### 1.7. ユーザ入力のキャプチャ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5tw2hGZOgmf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb94c5f-085b-4190-e7fe-d7da6fcaaf72"
      },
      "source": [
        "s = input(\"Enter some text: \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter some text: I'm Yuichi Yaguchi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzCMvhcGOgmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa26cfce-e2c8-4dfe-83cf-f5b64101484f"
      },
      "source": [
        "print(\"You typed\", len(word_tokenize(s)), \"words.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You typed 4 words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMRIGEs7Ogmg"
      },
      "source": [
        "### 1.8. NLP Pipeline\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "上図は本章で扱う処理をまとめた図である。概要として、『読み込み』、『タグ等の除去』、『本文の抽出』、『トークン化』、『語彙の正規化』を行う。  \n",
        "なお、正規化には、この後に説明する、StemmingやLemmatizing等の技術が必要となる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXb3toJ_Ogmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f546452c-e17a-4656-b953-64ff2d52ef29"
      },
      "source": [
        "raw = open('document.txt').read()\n",
        "print('raw type:',type(raw))\n",
        "tokens = word_tokenize(raw)\n",
        "print('tokens type:',type(tokens))\n",
        "words = [w.lower() for w in tokens]\n",
        "print('words type:',type(words))\n",
        "vocab = sorted(set(words))\n",
        "print('vocab type:',type(vocab))\n",
        "print(vocab[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw type: <class 'str'>\n",
            "tokens type: <class 'list'>\n",
            "words type: <class 'list'>\n",
            "vocab type: <class 'list'>\n",
            "['.', 'a', 'an', 'arrow', 'banana', 'flies', 'fruit', 'like', 'time']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-oHNmrXOgmh"
      },
      "source": [
        "Wrong Case 1: appendを文字列型に利用しようとするとエラー"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5xQZL-nOgmh"
      },
      "source": [
        "vocab.append('blog')\n",
        "raw.append('blog')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIoBbg8WOgmh"
      },
      "source": [
        "---------------------------------------------------------------------------\n",
        "AttributeError                            Traceback (most recent call last)\n",
        "<ipython-input-33-9b21c932c920> in <module>\n",
        "      1 vocab.append('blog')\n",
        "----> 2 raw.append('blog')\n",
        "\n",
        "AttributeError: 'str' object has no attribute 'append'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYLr20HSOgmh"
      },
      "source": [
        "Wrong Case 2: 文字列にリストを接合（できない）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc-R2PeeOgmh"
      },
      "source": [
        "query = 'Who knows?'\n",
        "beatles = ['john', 'paul', 'george', 'ringo']\n",
        "query + beatles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjiwA8gGOgmh"
      },
      "source": [
        "```\n",
        "---------------------------------------------------------------------------\n",
        "\n",
        "TypeError                                 Traceback (most recent call last)\n",
        "<ipython-input-34-67c1a27cd064> in <module>\n",
        "      1 query = 'Who knows?'\n",
        "      2 beatles = ['john', 'paul', 'george', 'ringo']\n",
        "----> 3 query + beatles\n",
        "\n",
        "TypeError: can only concatenate str (not \"list\") to str\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T48qivxmOgmi"
      },
      "source": [
        "\n",
        "\n",
        "- Unicode: 100万字以上のサポート。マルチバイト。\n",
        "\n",
        "### 2.1. Extract text from encoded file (3.3.2)\n",
        "ファイルからエンコードされたテキストを抽出。 (Unicode)  \n",
        "latin2 エンコードに変更"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-GuqC82Ogmi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c775a6a9-9a99-4ddf-9388-d9a5bee08f6d"
      },
      "source": [
        "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
        "f = open(path, encoding='latin2')\n",
        "for line in f:\n",
        "    line = line.strip()\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruska Biblioteka Państwowa. Jej dawne zbiory znane pod nazwą\n",
            "\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\n",
            "Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\n",
            "odnalezione po 1945 r. na terytorium Polski. Trafiły do Biblioteki\n",
            "Jagiellońskiej w Krakowie, obejmują ponad 500 tys. zabytkowych\n",
            "archiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BaUyCF6Ogmj"
      },
      "source": [
        "ユニコードエスケープを付与した形での表現 (頭に b' が付く。　\\\\u0144等のバイナリコードが付く）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BHTdiBHOgmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3bafe97-1960-4723-96fc-51104df77c8c"
      },
      "source": [
        "f = open(path, encoding='latin2')\n",
        "for line in f:\n",
        "    line = line.strip()\n",
        "    print(line.encode('unicode_escape'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Pruska Biblioteka Pa\\\\u0144stwowa. Jej dawne zbiory znane pod nazw\\\\u0105'\n",
            "b'\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez'\n",
            "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y'\n",
            "b'odnalezione po 1945 r. na terytorium Polski. Trafi\\\\u0142y do Biblioteki'\n",
            "b'Jagiello\\\\u0144skiej w Krakowie, obejmuj\\\\u0105 ponad 500 tys. zabytkowych'\n",
            "b'archiwali\\\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOEo7V51Ogmk"
      },
      "source": [
        "Pythonでは、通常の文字リテラルの前にuを付けることで、Unicode文字列リテラルとすることが出来る (u'a'など).   \n",
        "\n",
        "ordを利用して文字の序列を確認する: nacuteと呼ばれる文字を表現する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnZpACGVOgmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3696e2b-9dcb-4072-c508-7db4dabbf584"
      },
      "source": [
        "print(ord('ń'))\n",
        "nacute = '\\u0144'\n",
        "print(nacute)\n",
        "print(nacute.encode('utf8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "324\n",
            "ń\n",
            "b'\\xc5\\x84'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoqzJ5crOgmk"
      },
      "source": [
        "unicodedataモジュールはUnicode文字の属性を調べる機能を提供する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgfJ1UywOgml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c6c41c-04ee-4723-faf3-3300c25f8674"
      },
      "source": [
        "import unicodedata\n",
        "lines = open(path, encoding='latin2').readlines()\n",
        "line = lines[2]\n",
        "print(line.encode('unicode_escape'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y\\\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-cud3BHOgml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b18b3d-b855-4283-85b3-36ba589f08ca"
      },
      "source": [
        "for c in line:\n",
        "    if ord(c) > 127:\n",
        "        print('{} U+{:04x} {}'.format(c.encode('utf8'), ord(c), unicodedata.name(c)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'\\xc3\\xb3' U+00f3 LATIN SMALL LETTER O WITH ACUTE\n",
            "b'\\xc5\\x9b' U+015b LATIN SMALL LETTER S WITH ACUTE\n",
            "b'\\xc5\\x9a' U+015a LATIN CAPITAL LETTER S WITH ACUTE\n",
            "b'\\xc4\\x85' U+0105 LATIN SMALL LETTER A WITH OGONEK\n",
            "b'\\xc5\\x82' U+0142 LATIN SMALL LETTER L WITH STROKE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsEFrHcvOgml"
      },
      "source": [
        "Pythonの文字列メソッドとre(正規表現)おジュールがUnicode文字列をどのように扱うか？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6pOK6c_Ogml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98723ff9-0450-4262-ac19-2a0ff6039fc0"
      },
      "source": [
        "print('zosta: ', line.find('zosta\\u0142y'))\n",
        "line = line.lower()\n",
        "print('line: ', line)\n",
        "print('unicode: ', line.encode('unicode_escape'))\n",
        "import re\n",
        "m = re.search('\\u015b\\w*', line)\n",
        "print('searched: ', m.group())\n",
        "print(word_tokenize(line))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "zosta:  54\n",
            "line:  niemców pod koniec ii wojny światowej na dolny śląsk, zostały\n",
            "\n",
            "unicode:  b'niemc\\\\xf3w pod koniec ii wojny \\\\u015bwiatowej na dolny \\\\u015bl\\\\u0105sk, zosta\\\\u0142y\\\\n'\n",
            "searched:  światowej\n",
            "['niemców', 'pod', 'koniec', 'ii', 'wojny', 'światowej', 'na', 'dolny', 'śląsk', ',', 'zostały']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-xj5GLkOgmm"
      },
      "source": [
        "### 2.2. Pythonでローカルエンコーディングを使用する。\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnzlPcfaOgmm"
      },
      "source": [
        "## Lesson 3. Regular Expression to extract word pattern (3.4.)\n",
        "\n",
        "(★ Assingment Remark) 例えば、過去形 \"ed\" で終わる単語を見つける等と言った作業では、正規表現などを用いてマッチングさせることによって、その単語の情報を取得することが出来る。  \n",
        "凡そ、正規表現については、他の書籍も多くあることなので、是非そちらを参照されたいが、自然言語処理に必要と思われる部分を中心に例示する。\n",
        "\n",
        "下例では、語彙リストコーパスを用いる。予め、小文字にしておく。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7hHeavcOgmm"
      },
      "source": [
        "import re\n",
        "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()] # 単語を全て小文字にしておく"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-j940r1Ogmm"
      },
      "source": [
        "### 3.1. 基本的なメタキャラクタの利用\n",
        "\n",
        "例えば、過去形\"ed\" で終わる単語を探すためには、('ed$') という正規表現を利用することが出来る。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwmM-Y15Ogmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4440729-b819-48e7-8479-01e41341aa57"
      },
      "source": [
        "print([w for w in wordlist if re.search('ed$', w)][:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', 'abridged', 'abscessed', 'absconded', 'absorbed', 'abstracted', 'abstricted', 'accelerated', 'accepted', 'accidented', 'accoladed', 'accolated', 'accomplished', 'accosted', 'accredited', 'accursed', 'accused', 'accustomed', 'acetated', 'acheweed', 'aciculated', 'aciliated', 'acknowledged', 'acorned', 'acquainted', 'acquired', 'acquisited', 'acred', 'aculeated', 'addebted', 'added', 'addicted', 'addlebrained', 'addleheaded', 'addlepated', 'addorsed', 'adempted', 'adfected', 'adjoined', 'admired', 'admitted', 'adnexed', 'adopted', 'adossed', 'adreamed', 'adscripted', 'aduncated', 'advanced', 'advised', 'aeried', 'aethered', 'afeared', 'affected', 'affectioned', 'affined', 'afflicted', 'affricated', 'affrighted', 'affronted', 'aforenamed', 'afterfeed', 'aftershafted', 'afterthoughted', 'afterwitted', 'agazed', 'aged', 'agglomerated', 'aggrieved', 'agminated', 'agnamed', 'agonied', 'agreed', 'agueweed', 'ahungered', 'aiguilletted', 'ailweed', 'airbrained', 'airified', 'aiseweed', 'aisled', 'alarmed', 'alated', 'alimonied', 'aliped', 'alleyed', 'allied', 'alligatored', 'allseed', 'almsdeed', 'aloed', 'altared', 'alveolated', 'amazed', 'ameed']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd-CTXo8Ogmn"
      },
      "source": [
        "### Exercise Attendance:\n",
        "\n",
        "Please search 100 words which finished \"~ing\" as progressive form in the wordlist on previous example.\n",
        "進行形 \"ing\" で終わる単語を探して100件までリスト化してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMo1daTmOgmn"
      },
      "source": [
        "['abhorring', 'abiding', 'abounding', 'absorbing', 'abutting', 'accommodating', 'according', 'accounting', 'aching', 'acting', 'adeling', 'adjoining', 'admiring', 'adsmithing', 'advancing', 'advertising', 'affecting', 'afflicting', 'affronting', 'afluking', 'afterburning', 'aftercoming', 'afterking', 'afterplanting', 'afterreckoning', 'afterripening', 'afterspring', 'afterswarming', 'afterworking', 'aggravating', 'aging', 'agoing', 'agreeing', 'ailing', 'aiming', 'airing', 'aisling', 'alarming', 'allthing', 'alluring', 'almsgiving', 'alternating', 'amazing', 'ambling', 'ambuling', 'amusing', 'anglewing', 'angling', 'animating', 'annoying', 'antespring', 'antiagglutinating', 'antiboxing', 'anticlogging', 'anticoagulating', 'anticovenanting', 'anticreeping', 'antidancing', 'antidetonating', 'antidumping', 'antiexporting', 'antiflattering', 'antifoaming', 'antifouling', 'antifreezing', 'antigambling', 'antiganting', 'antihunting', 'antiking', 'antileveling', 'antilynching', 'antimixing', 'antioxidizing', 'antipooling', 'antipriming', 'antiprofiteering', 'antiquing', 'antiracing', 'antiradiating', 'antirebating', 'antirecruiting', 'antireforming', 'antishipping', 'antiskidding', 'antismoking', 'antisplitting', 'antispreading', 'antisquatting', 'antistalling', 'antivibrating', 'antling', 'anubing', 'anything', 'apeling', 'aping', 'appalling', 'appealing', 'appeasing', 'appraising', 'approaching']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcl2nk3UOgmn"
      },
      "source": [
        "'.' はワイルドカード記号。下の例は、8文字、3番目にj, 6番目にtが来る文字列を抽出。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ces2ajp9Ogmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bd5086-9edd-469a-9c30-be6bbccb76e1"
      },
      "source": [
        " print([w for w in wordlist if re.search('^..j..t..$', w)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', 'objectee', 'objector', 'rejecter', 'rejector', 'unjilted', 'unjolted', 'unjustly']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXbQS5AGOgmn"
      },
      "source": [
        "RangesとClosures\n",
        "![image.png](attachment:image.png)\n",
        " - T9システム: スマホに利用される。\n",
        " - キーストロークの同じシーケンスで入力された2つ以上の単語を、textonymsという。それを見つけられるか？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkE1VIwXOgmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d1c79a-5f85-495b-b9aa-f0464c303ecd"
      },
      "source": [
        "print([w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gold', 'golf', 'hold', 'hole']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iC597wkOgmo"
      },
      "source": [
        "Kleene Closure (Closure): '+'や'\\*' 記号で代表される、特定の記号列が連続して出現する生成パターンにマッチングするケース。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhwOCuFWOgmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f28ba42-12ab-4062-d34d-68321fdc844c"
      },
      "source": [
        "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
        "print([w for w in chat_words if re.search('^m+i+n+e+$', w)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine', 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptaGrCipOgmo"
      },
      "source": [
        "Class: '[',']' に囲まれた中の文字を1つの文字の候補として纏める。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BigkxWKjOgmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bedef6-766e-437b-ecea-1b234d6cb732"
      },
      "source": [
        "print([w for w in chat_words if re.search('^[ha]+$', w)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh', 'ahhahahaha', 'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'h', 'ha', 'haaa', 'hah', 'haha', 'hahaaa', 'hahah', 'hahaha', 'hahahaa', 'hahahah', 'hahahaha', 'hahahahaaa', 'hahahahahaha', 'hahahahahahaha', 'hahahahahahahahahahahahahahahaha', 'hahahhahah', 'hahhahahaha']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbg1ri6kOgmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ecc9b5-e9d7-4ae5-dd4f-6767e5018e89"
      },
      "source": [
        "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
        "print([w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)][:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['0.0085', '0.05', '0.1', '0.16', '0.2', '0.25', '0.28', '0.3', '0.4', '0.5', '0.50', '0.54', '0.56', '0.60', '0.7', '0.82', '0.84', '0.9', '0.95', '0.99', '1.01', '1.1', '1.125', '1.14', '1.1650', '1.17', '1.18', '1.19', '1.2', '1.20', '1.24', '1.25', '1.26', '1.28', '1.35', '1.39', '1.4', '1.457', '1.46', '1.49', '1.5', '1.50', '1.55', '1.56', '1.5755', '1.5805', '1.6', '1.61', '1.637', '1.64', '1.65', '1.7', '1.75', '1.76', '1.8', '1.82', '1.8415', '1.85', '1.8500', '1.9', '1.916', '1.92', '10.19', '10.2', '10.5', '107.03', '107.9', '109.73', '11.10', '11.5', '11.57', '11.6', '11.72', '11.95', '112.9', '113.2', '116.3', '116.4', '116.7', '116.9', '118.6', '12.09', '12.5', '12.52', '12.68', '12.7', '12.82', '12.97', '120.7', '1206.26', '121.6', '126.1', '126.15', '127.03', '129.91', '13.1', '13.15', '13.5', '13.50', '13.625']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6xm9Qi2Ogmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02004cb1-1c20-412c-84c7-f018829a0de7"
      },
      "source": [
        "print([w for w in wsj if re.search('^[A-Z]+\\$$', w)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['C$', 'US$']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zej8ti7fOgmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3db31fb-2dc1-4877-e4e7-d29ff9055c5e"
      },
      "source": [
        "print([w for w in wsj if re.search('^[0-9]{4}$', w)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1614', '1637', '1787', '1901', '1903', '1917', '1925', '1929', '1933', '1934', '1948', '1953', '1955', '1956', '1961', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1975', '1976', '1977', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2005', '2009', '2017', '2019', '2029', '3057', '8300']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZc8d0zkOgmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77d3e94-8ec1-4384-e5cb-a8a4a4ab7af3"
      },
      "source": [
        "print([w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['10-day', '10-lap', '10-year', '100-share', '12-point', '12-year', '14-hour', '15-day', '150-point', '190-point', '20-point', '20-stock', '21-month', '237-seat', '240-page', '27-year', '30-day', '30-point', '30-share', '30-year', '300-day', '36-day', '36-store', '42-year', '50-state', '500-stock', '52-week', '69-point', '84-month', '87-store', '90-day']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-Mq8QDwOgmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ed9812-4c81-4779-9ef8-5eda3f4a28fb"
      },
      "source": [
        "print([w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['black-and-white', 'bread-and-butter', 'father-in-law', 'machine-gun-toting', 'savings-and-loan']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC3brQIxOgmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e23b20-3c1d-4426-a441-4db691f6978d"
      },
      "source": [
        "print([w for w in wsj if re.search('(ed|ing)$', w)][:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['62%-owned', 'Absorbed', 'According', 'Adopting', 'Advanced', 'Advancing', 'Alfred', 'Allied', 'Annualized', 'Anything', 'Arbitrage-related', 'Arbitraging', 'Asked', 'Assuming', 'Atlanta-based', 'Baking', 'Banking', 'Beginning', 'Beijing', 'Being', 'Bermuda-based', 'Betting', 'Boeing', 'Broadcasting', 'Bucking', 'Buying', 'Calif.-based', 'Change-ringing', 'Citing', 'Concerned', 'Confronted', 'Conn.based', 'Consolidated', 'Continued', 'Continuing', 'Declining', 'Defending', 'Depending', 'Designated', 'Determining', 'Developed', 'Died', 'During', 'Encouraged', 'Encouraging', 'English-speaking', 'Estimated', 'Everything', 'Excluding', 'Exxon-owned', 'Faulding', 'Fed', 'Feeding', 'Filling', 'Filmed', 'Financing', 'Following', 'Founded', 'Fracturing', 'Francisco-based', 'Fred', 'Funded', 'Funding', 'Generalized', 'Germany-based', 'Getting', 'Guaranteed', 'Having', 'Heating', 'Heightened', 'Holding', 'Housing', 'Illuminating', 'Indeed', 'Indexing', 'Irving', 'Jersey-based', 'Judging', 'Knowing', 'Learning', 'Legislating', 'Leming', 'Limited', 'London-based', 'Manfred', 'Manufacturing', 'Melamed', 'Miami-based', 'Mich.-based', 'Mining', 'Minneapolis-based', 'Mo.-based', 'Mortgage-Backed', 'Moving', 'Muzzling', 'N.J.-based', 'NBC-owned', 'NIH-appointed', 'Named', 'No-Smoking']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqlvgrxOOgmr"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "上図: ワイルドカード、範囲、閉包を含む基本的な正規表現のメタキャラクタ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0N8F9lhOgnN"
      },
      "source": [
        "## Lesson 4. Useful Application of Regular Expression (3.5.)\n",
        "\n",
        "### 4.1. Extracting subsequence of a word\n",
        "\n",
        "re.findall()メソッドは、マッチングするすべての表現を見つける機能を持つ。これを用いて、Word Piecesの抽出を行う"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfFKrQlWOgnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2096e721-55a6-4e4e-9179-1fc40e7767f1"
      },
      "source": [
        "word = 'supercalifragilisticexpialidocious'\n",
        "print(re.findall(r'[aeiou]', word))\n",
        "print(len(re.findall(r'[aeiou]', word)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['u', 'e', 'a', 'i', 'a', 'i', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u']\n",
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OXptGaHOgnO"
      },
      "source": [
        "任意のテキスト内の2つ以上の連続した母音を全て調べ、それらの相対頻度を調べる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hYerO1xOgnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9add164-59ea-44d8-9dc6-f75503d16b27"
      },
      "source": [
        "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
        "fd = nltk.FreqDist(vs for word in wsj\n",
        "     for vs in re.findall(r'[aeiou]{2,}', word))\n",
        "print(fd.most_common(12))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('io', 549), ('ea', 476), ('ie', 331), ('ou', 329), ('ai', 261), ('ia', 253), ('ee', 217), ('oo', 174), ('ua', 109), ('au', 106), ('ue', 105), ('ui', 95)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8Aqez1bOgnP"
      },
      "source": [
        "### 4.2. Doing more with word pieces\n",
        "\n",
        " - 母音を抜いて、子音のみを集めることでも、英語は意味を簡単に理解できる。その操作を行う例"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWQDO-QNOgnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3bbbcb9-2a9f-4f0f-8bf3-c64d0866f279"
      },
      "source": [
        "regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'\n",
        "def compress(word):\n",
        "    pieces = re.findall(regexp, word)\n",
        "    return ''.join(pieces)\n",
        "english_udhr = nltk.corpus.udhr.words('English-Latin1')\n",
        "print(nltk.tokenwrap(english_udhr[:75]))\n",
        "print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Universal Declaration of Human Rights Preamble Whereas recognition of\n",
            "the inherent dignity and of the equal and inalienable rights of all\n",
            "members of the human family is the foundation of freedom , justice and\n",
            "peace in the world , Whereas disregard and contempt for human rights\n",
            "have resulted in barbarous acts which have outraged the conscience of\n",
            "mankind , and the advent of a world in which human beings shall enjoy\n",
            "freedom of speech and\n",
            "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\n",
            "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\n",
            "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\n",
            "rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\n",
            "and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n553rPkpOgnQ"
      },
      "source": [
        "正規表現を条件付き頻度分布との組み合わせで扱う: ロカトス語の単語から抜き出した全ての子音-母音の組み合わせ表。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dEeG2y4OgnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20fdcc5b-6a52-4ac5-d185-e81105690f6a"
      },
      "source": [
        "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n",
        "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
        "cfd = nltk.ConditionalFreqDist(cvs)\n",
        "print(cfd.tabulate())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    a   e   i   o   u \n",
            "k 418 148  94 420 173 \n",
            "p  83  31 105  34  51 \n",
            "r 187  63  84  89  79 \n",
            "s   0   0 100   2   1 \n",
            "t  47   8   0 148  37 \n",
            "v  93  27 105  48  49 \n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N64gSCVOgnQ"
      },
      "source": [
        "索引 (indexing): 'su'を含むすべての単語を返す、など。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEm4SaPkOgnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec032c9-b9af-4d24-aec4-02071227e2a6"
      },
      "source": [
        "cv_word_pairs = [(cv, w) for w in rotokas_words\n",
        "                         for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
        "cv_index = nltk.Index(cv_word_pairs)\n",
        "print(\"'su': \", cv_index['su'])\n",
        "print(\"'po': \", cv_index['po'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'su':  ['kasuari']\n",
            "'po':  ['kaapo', 'kaapopato', 'kaipori', 'kaiporipie', 'kaiporivira', 'kapo', 'kapoa', 'kapokao', 'kapokapo', 'kapokapo', 'kapokapoa', 'kapokapoa', 'kapokapora', 'kapokapora', 'kapokaporo', 'kapokaporo', 'kapokari', 'kapokarito', 'kapokoa', 'kapoo', 'kapooto', 'kapoovira', 'kapopaa', 'kaporo', 'kaporo', 'kaporopa', 'kaporoto', 'kapoto', 'karokaropo', 'karopo', 'kepo', 'kepoi', 'keposi', 'kepoto']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAR2FP1OOgnR"
      },
      "source": [
        "### 4.3. Find trunk of word\n",
        "\n",
        "Word Stemの検索: ing, ly, ed, ious, ies, ive, es, s, ment等の接尾語を取り除く場合\n",
        " - 最終的にはNLTK組み込みステマーを使用するが、この場所では正規表現で行ってみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5zBSA3_OgnS"
      },
      "source": [
        "def stem(word):\n",
        "     for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "        return word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt3gQ3VPOgnS"
      },
      "source": [
        "正規表現は全体にマッチするが、取得できるのは接尾語のみ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQYM0hG0OgnS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "223cedb5-1a7c-4714-fb73-40e0358c4afa"
      },
      "source": [
        "print(re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE8UstZnOgnT"
      },
      "source": [
        "?を追加して出力するマテリアルの選択"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCq2mpljOgnT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec543db-042e-4678-c1e8-15466fc75c30"
      },
      "source": [
        "print(re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['processing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvFYzbkjOgnU"
      },
      "source": [
        "Stemと接尾語に分けるために、正規表現の両方に括弧で囲む。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2EPe5TZOgnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8495ed-8a17-4046-ab10-bbac06653659"
      },
      "source": [
        "print(re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('process', 'ing')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-MSWY3POgnX"
      },
      "source": [
        "*がGreedyの一例: 誤ってsが出てくる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Peh6cvZPOgnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b144c132-e341-4be4-918a-ec8f70c16bda"
      },
      "source": [
        "print(re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('processe', 's')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsiLNqBGOgnY"
      },
      "source": [
        "貪欲でないVersion(*?)を利用すると、望むものを得られる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOHjgYNTOgnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3bb55a7-64de-4cde-e711-5e03ca527262"
      },
      "source": [
        "print(re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('process', 'es')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIO5-WUiOgnZ"
      },
      "source": [
        "空の接尾語を許可して2番目の括弧の内容をオプションにすることで機能。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35uuRi1NOgnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b87af2fc-1c24-421c-df15-9afe2dbf7e3e"
      },
      "source": [
        "print(re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('language', '')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCWMDV2xOgna"
      },
      "source": [
        "関数化し、テキスト全体に適用。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RU2flYVOgna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b84dad9-25e7-43b3-9178-422efae6eadd"
      },
      "source": [
        "def stem(word):\n",
        "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
        "    stem, suffix = re.findall(regexp, word)[0]\n",
        "    return stem\n",
        "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
        "is no basis for a system of government.  Supreme executive power derives from\n",
        "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
        "\n",
        "tokens = word_tokenize(raw)\n",
        "print([stem(t) for t in tokens])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'ly', 'in', 'pond', 'distribut', 'sword', 'i', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'Supreme', 'execut', 'power', 'deriv', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba8fYjSjOgna"
      },
      "source": [
        "### 4.4. トークン化テキストの検索\n",
        "特殊な種類の正規表現を利用してテキスト内の複数の単語を検索する。テキスト内の男性の全てのインスタンスを検索。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cpqGilXOgnb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c55f09b0-b805-4117-e658-6dea3525fd38"
      },
      "source": [
        "from nltk.corpus import gutenberg, nps_chat\n",
        "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
        "print(moby.findall(r\"<a> (<.*>) <man>\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
            "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
            "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
            "brave; brave; brave\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPkG_EoMOgnb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1816318d-f480-445b-a5de-aa43879ae03a"
      },
      "source": [
        "chat = nltk.Text(nps_chat.words()) #単語broで終わる3語の句を探す\n",
        "print(chat.findall(r\"<.*> <.*> <bro>\") )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you rule bro; telling you bro; u twizted bro\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fSFB3NkOgnd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb5b1a1-5cb6-4a21-f153-7a031897f73b"
      },
      "source": [
        "print(chat.findall(r\"<l.*>{3,}\")) # ｌで始まる単語が3語以上連続している部分を探す。"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
            "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT89yoW_Ogne"
      },
      "source": [
        "特定の単語を対象とした言語現象を研究対象とする場合、検索パターンを作るのは比較的容易。  \n",
        "例えば、x and other ys という正規表現で大きなテキストコーパスを検索することで、上位語を発見することが出来る。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muN2jrfjOgne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c24629-cf13-4cd5-d6f1-7c054874bccf"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
        "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "speed and other activities; water and other liquids; tomb and other\n",
            "landmarks; Statues and other monuments; pearls and other jewels;\n",
            "charts and other items; roads and other features; figures and other\n",
            "objects; military and other areas; demands and other factors;\n",
            "abstracts and other compilations; iron and other metals\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBl-tfNLOgnf"
      },
      "source": [
        "## Lesson 5. テキストの正規化 (3.6.)\n",
        "\n",
        "正規化:\n",
        " - lower() : 小文字に『正規化』する\n",
        " - stemming: 単語から全ての接辞（接頭・接尾語等）を取り除く。\n",
        " - lemmatize 見出し語化: 語の形を辞書に記述されている語形に変換する作業。\n",
        " \n",
        "以下のデータを利用する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5q49B1COgnf"
      },
      "source": [
        "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
        "is no basis for a system of government.  Supreme executive power derives from\n",
        "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
        "tokens = word_tokenize(raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sdNjXOeOgnf"
      },
      "source": [
        "### 5.1. Stemmers\n",
        "\n",
        "NLTKに含まれるStemmerを利用\n",
        " - PorterStemmer() 情報検索のために作られた古典的ステマー\n",
        " - LancasterStemmer() 英語用の古典的ステマー"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSgXeXSnOgng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "9fb27827-f1e0-46bc-9c98-8f03e5587216"
      },
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "lancaster = nltk.LancasterStemmer()\n",
        "print([porter.stem(t) for t in tokens])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-807ede4ebc60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mporter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlancaster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLancasterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVtvEnDHOgng"
      },
      "source": [
        "ステマーを用いたテキストのインデックス構築"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGMAmy5jOgng"
      },
      "source": [
        "class IndexedText(object):\n",
        "\n",
        "    def __init__(self, stemmer, text):\n",
        "        self._text = text\n",
        "        self._stemmer = stemmer\n",
        "        self._index = nltk.Index((self._stem(word), i)\n",
        "                                 for (i, word) in enumerate(text))\n",
        "\n",
        "    def concordance(self, word, width=40):\n",
        "        key = self._stem(word)\n",
        "        wc = int(width/4)                # words of context\n",
        "        for i in self._index[key]:\n",
        "            lcontext = ' '.join(self._text[i-wc:i])\n",
        "            rcontext = ' '.join(self._text[i:i+wc])\n",
        "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
        "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
        "            print(ldisplay, rdisplay)\n",
        "\n",
        "    def _stem(self, word):\n",
        "        return self._stemmer.stem(word).lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiBlF78DOgnh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "0a3f5e9f-ed19-4d08-9cd6-23f5cd11d032"
      },
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "grail = nltk.corpus.webtext.words('grail.txt')\n",
        "text = IndexedText(porter, grail)\n",
        "text.concordance('lie')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-18e31c37e7bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mporter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgrail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grail.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndexedText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mporter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcordance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lie'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfiHExV-Ognh"
      },
      "source": [
        "### 5.2. Lemmatization\n",
        "見出し語化: Wordnetの字句解析ツールは、結果の単語が辞書に含まれている場合にのみ、接尾語を削除します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Op3q_igOgnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64014194-e62a-4653-dd31-15d58cd1cee5"
      },
      "source": [
        "wnl = nltk.WordNetLemmatizer()\n",
        "print([wnl.lemmatize(t) for t in tokens])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40bHk3JfOgni"
      },
      "source": [
        "## Lesson 6. Formatting: From Lists to String (3.9.)\n",
        "\n",
        "### 6.1. リストから文字列\n",
        "リストから文字列に変換すると、より良い出力が出来る可能性がある。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEoaSjTeOgni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cbbd42b-e7ca-417d-c205-5cb044b40a93"
      },
      "source": [
        "silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']\n",
        "print('blank: ', ' '.join(silly))\n",
        "print('semicolon: ', ';'.join(silly))\n",
        "print('tight: ', ''.join(silly))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "blank:  We called him Tortoise because he taught us .\n",
            "semicolon:  We;called;him;Tortoise;because;he;taught;us;.\n",
            "tight:  WecalledhimTortoisebecausehetaughtus.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR3-dOveOgni"
      },
      "source": [
        "### 6.2. 文字列と整形"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFOUY4HGOgni"
      },
      "source": [
        "print文: Pythonがオブジェクトの内容を人間に見やすいように整形する方法。  \n",
        "プロンプトで変数の名前を直接指定する方法←簡単だが、積極的には出力されない。  \n",
        "どちらも単なる文字列。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF5trkKuOgni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c94224-8abe-43f4-df3c-cc8c18321f08"
      },
      "source": [
        "word = 'cat'\n",
        "sentence = \"\"\"hello\n",
        "world\"\"\"\n",
        "print(word)\n",
        "print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat\n",
            "hello\n",
            "world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bKoQ3HhOgnj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a5b25b8f-d907-4245-a772-6cc8ae507856"
      },
      "source": [
        "word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dXKNChL2Ognj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2685cd29-df5c-472b-b166-8d3967fe6241"
      },
      "source": [
        "sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hello\\nworld'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Wxr38GOgnk"
      },
      "source": [
        "プリント文を使ってフォーマッティングをすると面倒くさい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZh90YgrOgnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1f3650-a5f9-4ea0-9867-5767e8c078e9"
      },
      "source": [
        "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
        "for word in sorted(fdist):\n",
        "    print(word, '->', fdist[word], end='; ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat -> 3; dog -> 4; snake -> 1; "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo-_y6FGOgnl"
      },
      "source": [
        "文字列整形式を利用すると非常に使いやすい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz9fassdOgnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2811106c-a4ec-4ad1-f5db-47528c841f6f"
      },
      "source": [
        "for word in sorted(fdist):\n",
        "   print('{}->{};'.format(word, fdist[word]), end=' ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat->3; dog->4; snake->1; "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztlSYBG-Ognm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20165d17-007b-416b-bfbd-d2709eab65dc"
      },
      "source": [
        "print('{}->{};'.format ('cat', 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat->3;\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKrw71fNOgno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0471dd69-d8a2-4fa0-9a45-735b91b8d3c8"
      },
      "source": [
        "print('{}->'.format('cat'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat->\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yQu_FESOgnp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4af7abb-fe77-44d2-eea0-3a136c22d3e6"
      },
      "source": [
        "print('{}'.format(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rNLvvjkOgnq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902ed5ee-2cc4-4ae9-a474-d40f626b2513"
      },
      "source": [
        "print('I want a {} right now'.format('coffee'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I want a coffee right now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcrlEimFOgnq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25630de-a4c5-4ae9-d2fd-da3e59013a2e"
      },
      "source": [
        "print('{} wants a {} {}'.format ('Lee', 'sandwich', 'for lunch'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lee wants a sandwich for lunch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myKvopg0Ogn0"
      },
      "source": [
        "'{} wants a {} {}'.format ('sandwich', 'for lunch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0nnIwNNOgn1"
      },
      "source": [
        "---------------------------------------------------------------------------\n",
        "IndexError                                Traceback (most recent call last)\n",
        "<ipython-input-92-339b7c04a9c2> in <module>\n",
        "----> 1 '{} wants a {} {}'.format ('sandwich', 'for lunch')\n",
        "\n",
        "IndexError: tuple index out of range"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbWH0ibtOgn9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e04151-1730-4b93-856c-b520a3bfd583"
      },
      "source": [
        "print('{} wants a {}'.format ('Lee', 'sandwich', 'for lunch'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lee wants a sandwich\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hx2UVsiOgn9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1d0ff2-4a32-4941-92fa-527fb3e654bc"
      },
      "source": [
        "print('from {1} to {0}'.format('A', 'B'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from B to A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhQzpdIvOgn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f66f655-8e59-4fe2-ef3f-e1142045ac87"
      },
      "source": [
        "template = 'Lee wants a {} right now'\n",
        "menu = ['sandwich', 'spam fritter', 'pancake']\n",
        "for snack in menu:\n",
        "    print(template.format(snack))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lee wants a sandwich right now\n",
            "Lee wants a spam fritter right now\n",
            "Lee wants a pancake right now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYMnNLYTOgn-"
      },
      "source": [
        "### 6.3. 値を並べる\n",
        "\n",
        "Lining things Up: パディングして出力"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqmxt5e6Ogn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce7c1775-5ec9-4032-92df-09cc3133077b"
      },
      "source": [
        "print('{:6}'.format(41))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-Y1wp1kOgn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e621c0fa-5272-4236-bf43-306fb296984c"
      },
      "source": [
        "print('{:<6}' .format(41))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlQdb_N8Ogn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb1cf81-4939-4870-bf68-e802d6918a0a"
      },
      "source": [
        "print('{:6}'.format('dog'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnZi0QDbOgoA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf784e4e-0819-45be-9cbc-3a4173f45562"
      },
      "source": [
        "print('{:>6}'.format('dog') )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvV6Xg9BOgoC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "427abfef-8441-46e3-f2bb-b53fa7bcb374"
      },
      "source": [
        "import math\n",
        "print('{:.4f}'.format(math.pi))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.1416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs-KN_JeOgoC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b806c587-980d-456c-962f-53a28a668384"
      },
      "source": [
        "count, total = 3205, 9375\n",
        "print(\"accuracy for {} words: {:.4%}\".format(total, count / (1.0* total)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy for 9375 words: 34.1867%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D8q-NGROgoD"
      },
      "source": [
        "def tabulate(cfdist, words, categories):\n",
        "    print('{:16}'.format('Category'), end=' ')                    # column headings\n",
        "    for word in words:\n",
        "        print('{:>6}'.format(word), end=' ')\n",
        "    print()\n",
        "    for category in categories:\n",
        "        print('{:16}'.format(category), end=' ')                  # row heading\n",
        "        for word in words:                                        # for each word\n",
        "            print('{:6}'.format(cfdist[category][word]), end=' ') # print table cell\n",
        "        print()                                                   # end the row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsfpxdXXOgoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2614c43-fedf-4282-ad93-8e6de5fb65a7"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "          (genre, word)\n",
        "          for genre in brown.categories()\n",
        "          for word in brown.words(categories=genre))\n",
        "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "tabulate(cfd, modals, genres)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Category            can  could    may  might   must   will \n",
            "news                 93     86     66     38     50    389 \n",
            "religion             82     59     78     12     54     71 \n",
            "hobbies             268     58    131     22     83    264 \n",
            "science_fiction      16     49      4     12      8     16 \n",
            "romance              74    193     11     51     45     43 \n",
            "humor                16     30      8      8      9     13 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AeT3U26OgoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794af349-401d-461e-9a5e-4993a2bd527a"
      },
      "source": [
        "print('{:{width}}'.format('Monty Python', width=15))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Monty Python   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unIdAyXEOgoE"
      },
      "source": [
        "### 6.4. ファイル出力"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zLyKd8bOgoE"
      },
      "source": [
        "output_file = open('output.txt', 'w')\n",
        "words = set(nltk.corpus.genesis.words('english-kjv.txt'))\n",
        "for word in sorted(words):\n",
        "    print(word, file=output_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1vfVQnQOgoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adde55fb-2b3f-4014-c276-b777fe00f475"
      },
      "source": [
        "len(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2789"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFqP0imDOgoF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fea770b1-4757-49c1-cee6-7036a9918817"
      },
      "source": [
        "str(len(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2789'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQlAlrXJOgoF"
      },
      "source": [
        "print(str(len(words)), file=output_file)\n",
        "output_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCeafhyjOgoG"
      },
      "source": [
        "### 6.5. Text Wrapping: %s, %d等"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WYEzvdSOgoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab9a223-1f83-4b65-ba29-955b0486cc07"
      },
      "source": [
        "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',\n",
        "         'more', 'is', 'said', 'than', 'done', '.']\n",
        "for word in saying:\n",
        "    print(word, '(' + str(len(word)) + '),', end=' ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), than (4), done (4), . (1), "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVnDNU-WOgoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019007e0-9ff8-459c-b6f4-989799ca8aaf"
      },
      "source": [
        "from textwrap import fill\n",
        "format = '%s (%d),'\n",
        "pieces = [format % (word, len(word)) for word in saying]\n",
        "output = ' '.join(pieces)\n",
        "wrapped = fill(output)\n",
        "print(wrapped)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more\n",
            "(4), is (2), said (4), than (4), done (4), . (1),\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}